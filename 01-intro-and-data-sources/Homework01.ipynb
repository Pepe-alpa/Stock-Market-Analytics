{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "07ac3c2c",
   "metadata": {},
   "source": [
    "# Module 1 Homework"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4423ddb9",
   "metadata": {},
   "source": [
    "### Question 1: [Index] S&P 500 Stocks Added to the Index\n",
    "\n",
    "**Which year had the highest number of additions?**\n",
    "\n",
    "Using the list of S&P 500 companies from Wikipedia's [S&P 500 companies page](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies), download the data including the year each company was added to the index.\n",
    "\n",
    "Hint: you can use [pandas.read_html](https://pandas.pydata.org/docs/reference/api/pandas.read_html.html) to scrape the data into a DataFrame.\n",
    "\n",
    "Steps:\n",
    "1. Create a DataFrame with company tickers, names, and the year they were added.\n",
    "2. Extract the year from the addition date and calculate the number of stocks added each year.\n",
    "3. Which year had the highest number of additions (1957 doesn't count, as it was the year when the S&P 500 index was founded)? Write down this year as your answer (the most recent one, if you have several records).\n",
    "\n",
    "*Context*: \n",
    "> \"Following the announcement, all four new entrants saw their stock prices rise in extended trading on Friday\" - recent examples of S&P 500 additions include DASH, WSM, EXE, TKO in 2025 ([Nasdaq article](https://www.nasdaq.com/articles/sp-500-reshuffle-dash-tko-expe-wsm-join-worth-buying)).\n",
    "\n",
    "*Additional*: How many current S&P 500 stocks have been in the index for more than 20 years? When stocks are added to the S&P 500, they usually experience a price bump as investors and index funds buy shares following the announcement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a37eba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a03d1b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://en.wikipedia.org/wiki/List_of_S%26P_500_companies\"\n",
    "table = pd.read_html(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0409fb7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Security</th>\n",
       "      <th>GICS Sector</th>\n",
       "      <th>GICS Sub-Industry</th>\n",
       "      <th>Headquarters Location</th>\n",
       "      <th>Date added</th>\n",
       "      <th>CIK</th>\n",
       "      <th>Founded</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>MMM</td>\n",
       "      <td>3M</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Industrial Conglomerates</td>\n",
       "      <td>Saint Paul, Minnesota</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>66740</td>\n",
       "      <td>1902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AOS</td>\n",
       "      <td>A. O. Smith</td>\n",
       "      <td>Industrials</td>\n",
       "      <td>Building Products</td>\n",
       "      <td>Milwaukee, Wisconsin</td>\n",
       "      <td>2017-07-26</td>\n",
       "      <td>91142</td>\n",
       "      <td>1916</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ABT</td>\n",
       "      <td>Abbott Laboratories</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Health Care Equipment</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>1957-03-04</td>\n",
       "      <td>1800</td>\n",
       "      <td>1888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ABBV</td>\n",
       "      <td>AbbVie</td>\n",
       "      <td>Health Care</td>\n",
       "      <td>Biotechnology</td>\n",
       "      <td>North Chicago, Illinois</td>\n",
       "      <td>2012-12-31</td>\n",
       "      <td>1551152</td>\n",
       "      <td>2013 (1888)</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ACN</td>\n",
       "      <td>Accenture</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>IT Consulting &amp; Other Services</td>\n",
       "      <td>Dublin, Ireland</td>\n",
       "      <td>2011-07-06</td>\n",
       "      <td>1467373</td>\n",
       "      <td>1989</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol             Security             GICS Sector  \\\n",
       "0    MMM                   3M             Industrials   \n",
       "1    AOS          A. O. Smith             Industrials   \n",
       "2    ABT  Abbott Laboratories             Health Care   \n",
       "3   ABBV               AbbVie             Health Care   \n",
       "4    ACN            Accenture  Information Technology   \n",
       "\n",
       "                GICS Sub-Industry    Headquarters Location  Date added  \\\n",
       "0        Industrial Conglomerates    Saint Paul, Minnesota  1957-03-04   \n",
       "1               Building Products     Milwaukee, Wisconsin  2017-07-26   \n",
       "2           Health Care Equipment  North Chicago, Illinois  1957-03-04   \n",
       "3                   Biotechnology  North Chicago, Illinois  2012-12-31   \n",
       "4  IT Consulting & Other Services          Dublin, Ireland  2011-07-06   \n",
       "\n",
       "       CIK      Founded  \n",
       "0    66740         1902  \n",
       "1    91142         1916  \n",
       "2     1800         1888  \n",
       "3  1551152  2013 (1888)  \n",
       "4  1467373         1989  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = table[0]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "386fcab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Date added'] = pd.to_datetime(df['Date added'], errors='coerce')\n",
    "\n",
    "df['added_year'] = df['Date added'].dt.year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a7b1615a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1957    53\n",
      "2016    23\n",
      "2017    23\n",
      "2019    22\n",
      "2008    17\n",
      "2022    16\n",
      "2024    16\n",
      "2023    15\n",
      "2021    15\n",
      "2012    14\n",
      "1997    14\n",
      "2015    14\n",
      "2018    14\n",
      "2002    13\n",
      "2007    12\n",
      "2009    12\n",
      "2020    12\n",
      "1998    11\n",
      "1976    11\n",
      "2013    10\n",
      "2006    10\n",
      "2011    10\n",
      "2000     9\n",
      "1999     9\n",
      "2010     9\n",
      "2001     8\n",
      "2014     8\n",
      "1994     7\n",
      "2005     7\n",
      "1995     7\n",
      "1985     7\n",
      "2004     6\n",
      "1982     5\n",
      "1989     5\n",
      "2025     5\n",
      "1984     5\n",
      "2003     5\n",
      "1988     4\n",
      "1986     3\n",
      "1993     3\n",
      "1983     3\n",
      "1981     3\n",
      "1992     3\n",
      "1980     3\n",
      "1972     2\n",
      "1987     2\n",
      "1969     2\n",
      "1996     2\n",
      "1965     2\n",
      "1973     2\n",
      "1975     2\n",
      "1970     2\n",
      "1979     2\n",
      "1974     1\n",
      "1991     1\n",
      "1964     1\n",
      "1978     1\n",
      "Name: added_year, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "count_by_year = df['added_year'].value_counts()\n",
    "\n",
    "print(count_by_year)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8c69d3e",
   "metadata": {},
   "source": [
    "### Question 2. [Macro] Indexes YTD (as of 1 May 2025)\n",
    "\n",
    "**How many indexes (out of 10) have better year-to-date returns than the US (S&P 500) as of May 1, 2025?**\n",
    "\n",
    "Using Yahoo Finance World Indices data, compare the year-to-date (YTD) performance (1 January-1 May 2025) of major stock market indexes for the following countries:\n",
    "* United States - S&P 500 (^GSPC)\n",
    "* China - Shanghai Composite (000001.SS)\n",
    "* Hong Kong - HANG SENG INDEX (^HSI)\t\n",
    "* Australia - S&P/ASX 200 (^AXJO)\n",
    "* India - Nifty 50 (^NSEI)\n",
    "* Canada - S&P/TSX Composite (^GSPTSE)\n",
    "* Germany - DAX (^GDAXI)\n",
    "* United Kingdom - FTSE 100 (^FTSE)\n",
    "* Japan - Nikkei 225 (^N225)\n",
    "* Mexico - IPC Mexico (^MXX)\n",
    "* Brazil - Ibovespa (^BVSP)\n",
    "\n",
    "*Hint*: use start_date='2025-01-01' and end_date='2025-05-01' when downloading daily data in yfinance\n",
    "\n",
    "Context: \n",
    "> [Global Valuations: Who's Cheap, Who's Not?](https://simplywall.st/article/beyond-the-us-global-markets-after-yet-another-tariff-update) article suggests \"Other regions may be growing faster than the US and you need to diversify.\"\n",
    "\n",
    "Reference: Yahoo Finance World Indices - https://finance.yahoo.com/world-indices/\n",
    "\n",
    "*Additional*: How many of these indexes have better returns than the S&P 500 over 3, 5, and 10 year periods? Do you see the same trend?\n",
    "Note: For simplicity, ignore currency conversion effects.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5f7baf5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "a43fe27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  11 of 11 completed\n"
     ]
    }
   ],
   "source": [
    "indexes = {\n",
    "    \"USA (S&P 500)\": \"^GSPC\",\n",
    "    \"China (Shanghai)\": \"000001.SS\",\n",
    "    \"Hong Kong (Hang Seng)\": \"^HSI\",\n",
    "    \"Australia (ASX 200)\": \"^AXJO\",\n",
    "    \"India (Nifty 50)\": \"^NSEI\",\n",
    "    \"Canada (TSX)\": \"^GSPTSE\",\n",
    "    \"Germany (DAX)\": \"^GDAXI\",\n",
    "    \"UK (FTSE 100)\": \"^FTSE\",\n",
    "    \"Japan (Nikkei 225)\": \"^N225\",\n",
    "    \"Mexico (IPC)\": \"^MXX\",\n",
    "    \"Brazil (Ibovespa)\": \"^BVSP\"\n",
    "}\n",
    "\n",
    "\n",
    "dax_daily = yf.download(list(indexes.values()), start = '2025-01-01', end = '2025-05-01')['Close']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ac21b2cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexes with better returns than S&P 500: 9 of 10\n",
      "\n",
      "Sorted returns:\n",
      "                       YTD Return\n",
      "Mexico (IPC)             0.130494\n",
      "Hong Kong (Hang Seng)    0.127200\n",
      "Brazil (Ibovespa)        0.124387\n",
      "Germany (DAX)            0.123464\n",
      "UK (FTSE 100)            0.028426\n",
      "India (Nifty 50)         0.024904\n",
      "China (Shanghai)         0.005048\n",
      "Canada (TSX)            -0.002261\n",
      "Australia (ASX 200)     -0.009145\n",
      "USA (S&P 500)           -0.051033\n",
      "Japan (Nikkei 225)      -0.082979\n"
     ]
    }
   ],
   "source": [
    "# Calculate returns YTD\n",
    "returns = {}\n",
    "\n",
    "for name, symbol in indexes.items():\n",
    "    df = dax_daily[symbol].dropna()\n",
    "    if len(df) > 1:\n",
    "        ytd_return = (df.iloc[-1] - df.iloc[0]) / df.iloc[0]\n",
    "        returns[name] = ytd_return\n",
    "\n",
    "returns_df = pd.DataFrame.from_dict(returns, orient='index', columns=['YTD Return'])\n",
    "returns_df.sort_values(by='YTD Return', ascending=False, inplace=True)\n",
    "\n",
    "sp500_return = returns_df.loc[\"USA (S&P 500)\", \"YTD Return\"]\n",
    "better_than_sp500 = (returns_df['YTD Return'] > sp500_return).sum()\n",
    "\n",
    "print(f\"Indexes with better returns than S&P 500: {better_than_sp500} of {len(returns_df)-1}\")\n",
    "print(\"\\nSorted returns:\")\n",
    "print(returns_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce683b8a",
   "metadata": {},
   "source": [
    "### Question 3. [Index] S&P 500 Market Corrections Analysis\n",
    "\n",
    "\n",
    "**Calculate the median duration (in days) of significant market corrections in the S&P 500 index.**\n",
    "\n",
    "For this task, define a correction as an event when a stock index goes down by **more than 5%** from the closest all-time high maximum.\n",
    "\n",
    "Steps:\n",
    "1. Download S&P 500 historical data (1950-present) using yfinance\n",
    "2. Identify all-time high points (where price exceeds all previous prices)\n",
    "3. For each pair of consecutive all-time highs, find the minimum price in between\n",
    "4. Calculate drawdown percentages: (high - low) / high × 100\n",
    "5. Filter for corrections with at least 5% drawdown\n",
    "6. Calculate the duration in days for each correction period\n",
    "7. Determine the 25th, 50th (median), and 75th percentiles for correction durations\n",
    "\n",
    "*Context:* \n",
    "> * Investors often wonder about the typical length of market corrections when deciding \"when to buy the dip\" ([Reddit discussion](https://www.reddit.com/r/investing/comments/1jrqnte/when_are_you_buying_the_dip/?rdt=64135)).\n",
    "\n",
    "> * [A Wealth of Common Sense - How Often Should You Expect a Stock Market Correction?](https://awealthofcommonsense.com/2022/01/how-often-should-you-expect-a-stock-market-correction/)\n",
    "\n",
    "*Hint (use this data to compare with your results)*: Here is the list of top 10 largest corrections by drawdown:\n",
    "* 2007-10-09 to 2009-03-09: 56.8% drawdown over 517 days\n",
    "* 2000-03-24 to 2002-10-09: 49.1% drawdown over 929 days\n",
    "* 1973-01-11 to 1974-10-03: 48.2% drawdown over 630 days\n",
    "* 1968-11-29 to 1970-05-26: 36.1% drawdown over 543 days\n",
    "* 2020-02-19 to 2020-03-23: 33.9% drawdown over 33 days\n",
    "* 1987-08-25 to 1987-12-04: 33.5% drawdown over 101 days\n",
    "* 1961-12-12 to 1962-06-26: 28.0% drawdown over 196 days\n",
    "* 1980-11-28 to 1982-08-12: 27.1% drawdown over 622 days\n",
    "* 2022-01-03 to 2022-10-12: 25.4% drawdown over 282 days\n",
    "* 1966-02-09 to 1966-10-07: 22.2% drawdown over 240 days"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "183b269c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6290a818",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "# Download S&P 500 data\n",
    "\n",
    "data = yf.download(\"^GSPC\", start=\"1950-01-01\", end=\"2025-05-01\")\n",
    "\n",
    "if isinstance(data.columns, pd.MultiIndex):\n",
    "    data.columns = ['_'.join(col).strip() if col[1] else col[0] for col in data.columns]\n",
    "\n",
    "data = data.reset_index()\n",
    "\n",
    "data.rename(columns={'Close': 'Close_GSPC', 'Close_^GSPC': 'Close_GSPC'}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "dc42fbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Date', 'Close_GSPC', 'High_^GSPC', 'Low_^GSPC', 'Open_^GSPC',\n",
       "       'Volume_^GSPC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "865ff7fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All time high points\n",
    "data['All_Time_High'] = data['Close_GSPC'].cummax()\n",
    "high_points = data[data['Close_GSPC'] == data['All_Time_High']].copy().reset_index()\n",
    "\n",
    "# Find the minimum in between for each consecutive pair of all-time-highs\n",
    "min_between_highs = []\n",
    "for i in range(len(high_points) - 1):\n",
    "    start_idx = high_points.loc[i, 'index']\n",
    "    end_idx = high_points.loc[i+1, 'index']\n",
    "\n",
    "    if end_idx - start_idx > 1:\n",
    "        between = data.loc[start_idx+1:end_idx-1]\n",
    "        min_row = between.loc[between['Close_GSPC'].idxmin()]\n",
    "        min_between_highs.append({\n",
    "            'from_date': data.loc[start_idx, 'Date'],\n",
    "            'to_date': min_row['Date'],\n",
    "            'high_price': data.loc[start_idx, 'Close_GSPC'],\n",
    "            'min_price': min_row['Close_GSPC'],\n",
    "            'min_date': min_row['Date']\n",
    "        })\n",
    "\n",
    "min_df = pd.DataFrame(min_between_highs)\n",
    "\n",
    "# Calculate Drawdown percentages\n",
    "min_df['drawdown_pct'] = (min_df['high_price'] - min_df['min_price']) / min_df['high_price'] * 100\n",
    "\n",
    "# Filter for corrections with at least 5% drawdown\n",
    "corrections = min_df[min_df['drawdown_pct'] >= 5].copy()\n",
    "\n",
    "# Calculate duration in days for each correction\n",
    "corrections['durations_days'] = (corrections['to_date'] - corrections['from_date']).dt.days\n",
    "\n",
    "# Calculate percentiles for duration\n",
    "percentiles = corrections['durations_days'].quantile([0.25, 0.5, 0.75]).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "9a8ba6ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     from_date    to_date  drawdown_pct  durations_days\n",
      "448 2007-10-09 2009-03-09     56.775388             517\n",
      "443 2000-03-24 2002-10-09     49.146948             929\n",
      "206 1973-01-11 1974-10-03     48.203593             630\n",
      "193 1968-11-29 1970-05-26     36.061641             543\n",
      "574 2020-02-19 2020-03-23     33.924960              33\n",
      "292 1987-08-25 1987-12-04     33.509515             101\n",
      "133 1961-12-12 1962-06-26     27.973568             196\n",
      "219 1980-11-28 1982-08-12     27.113582             622\n",
      "620 2022-01-03 2022-10-12     25.425097             282\n",
      "176 1966-02-09 1966-10-07     22.177335             240\n"
     ]
    }
   ],
   "source": [
    "top = corrections.sort_values(by='drawdown_pct', ascending=False).head(10)\n",
    "\n",
    "print(top[['from_date', 'to_date', 'drawdown_pct', 'durations_days']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "61647def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The median is 39.0\n"
     ]
    }
   ],
   "source": [
    "print(f\"The median is {percentiles[0.5]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e064ae7e",
   "metadata": {},
   "source": [
    "### Question 4.  [Stocks] Earnings Surprise Analysis for Amazon (AMZN)\n",
    "\n",
    "\n",
    "**Calculate the median 2-day percentage change in stock prices following positive earnings surprises days.**\n",
    "\n",
    "Steps:\n",
    "1. Load earnings data from CSV ([ha1_Amazon.csv](ha1_Amazon.csv)) containing earnings dates, EPS estimates, and actual EPS. Make sure you are using the correct delimiter to read the data, such as in this command ```python pandas.read_csv(\"ha1_Amazon.csv\", delimiter=';') ```\n",
    "2. Download complete historical price data using yfinance\n",
    "3. Calculate 2-day percentage changes for all historical dates: for each sequence of 3 consecutive trading days (Day 1, Day 2, Day 3), compute the *return* as Close_Day3 / Close_Day1 - 1. (Assume Day 2 may correspond to the earnings announcement.)\n",
    "4. Identify positive earnings surprises (where \"actual EPS > estimated EPS\" OR \"Surprise (%)>0\")\n",
    "5. Calculate 2-day percentage changes following positive earnings surprises. Show your answer in % (closest number to the 2nd digit): *return* * 100.0\n",
    "6. (Optional) Compare the median 2-day percentage change for positive surprises vs. all historical dates. Do you see the difference?\n",
    "\n",
    "Context: Earnings announcements, especially when they exceed analyst expectations, can significantly impact stock prices in the short term.\n",
    "\n",
    "Reference: Yahoo Finance earnings calendar - https://finance.yahoo.com/calendar/earnings?symbol=AMZN\n",
    "\n",
    "*Additional*: Is there a correlation between the magnitude of the earnings surprise and the stock price reaction? Does the market react differently to earnings surprises during bull vs. bear markets?)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "ac1a965b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Symbol</th>\n",
       "      <th>Company</th>\n",
       "      <th>Earnings Date</th>\n",
       "      <th>EPS Estimate</th>\n",
       "      <th>Reported EPS</th>\n",
       "      <th>Surprise (%)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com Inc</td>\n",
       "      <td>April 29, 2026 at 6 AM EDT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com Inc</td>\n",
       "      <td>February 4, 2026 at 4 PM EST</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com Inc</td>\n",
       "      <td>October 29, 2025 at 6 AM EDT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com Inc</td>\n",
       "      <td>July 30, 2025 at 4 PM EDT</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com, Inc.</td>\n",
       "      <td>May 1, 2025 at 4 PM EDT</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.59</td>\n",
       "      <td>+16.74</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Symbol           Company                 Earnings Date  EPS Estimate  \\\n",
       "0   AMZN    Amazon.com Inc    April 29, 2026 at 6 AM EDT           NaN   \n",
       "1   AMZN    Amazon.com Inc  February 4, 2026 at 4 PM EST           NaN   \n",
       "2   AMZN    Amazon.com Inc  October 29, 2025 at 6 AM EDT           NaN   \n",
       "3   AMZN    Amazon.com Inc     July 30, 2025 at 4 PM EDT           NaN   \n",
       "4   AMZN  Amazon.com, Inc.       May 1, 2025 at 4 PM EDT          0.36   \n",
       "\n",
       "   Reported EPS Surprise (%)  \n",
       "0           NaN            -  \n",
       "1           NaN            -  \n",
       "2           NaN            -  \n",
       "3           NaN            -  \n",
       "4          0.59       +16.74  "
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"ha1_Amazon.csv\", delimiter=';', encoding='utf-8')\n",
    "\n",
    "df['EPS Estimate'] = pd.to_numeric(df['EPS Estimate'].str.replace(r'[^\\d\\.\\-]', '', regex=True), errors='coerce')\n",
    "df['Reported EPS'] = pd.to_numeric(df['Reported EPS'].str.replace(r'[^\\d\\.\\-]', '', regex=True), errors='coerce')\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9170686f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%***********************]  1 of 1 completed\n"
     ]
    }
   ],
   "source": [
    "amzn = yf.download(\"AMZN\", start='2020-01-01')\n",
    "\n",
    "if isinstance(amzn.columns, pd.MultiIndex):\n",
    "    amzn.columns = [\n",
    "        f\"{col[0]}_{col[1]}\" if col[1] else col[0]\n",
    "        for col in amzn.columns\n",
    "    ]\n",
    "\n",
    "amzn = amzn.reset_index()\n",
    "amzn['Date'] = pd.to_datetime(amzn['Date'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "63d394eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Date', 'Close_AMZN', 'High_AMZN', 'Low_AMZN', 'Open_AMZN', 'Volume_AMZN']\n"
     ]
    }
   ],
   "source": [
    "print(amzn.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cdbb231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute 2-day percentage returns\n",
    "amzn['Return_2d'] = (\n",
    "    amzn['Close_AMZN'].astype(float)\n",
    "    .shift(-2)\n",
    "    .div(amzn['Close_AMZN'].astype(float)) - 1\n",
    ")\n",
    "\n",
    "amzn = amzn[amzn['Return_2d'].notna()].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "52e8ef46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cachu\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EDT identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n",
      "c:\\Users\\cachu\\anaconda3\\lib\\site-packages\\dateutil\\parser\\_parser.py:1207: UnknownTimezoneWarning: tzname EST identified but not understood.  Pass `tzinfos` argument in order to correctly return a timezone-aware datetime.  In a future version, this will raise an exception.\n",
      "  warnings.warn(\"tzname {tzname} identified but not understood.  \"\n"
     ]
    }
   ],
   "source": [
    "# Convert Earnings Date to datetime\n",
    "df['Earnings Date'] = pd.to_datetime(df['Earnings Date'], errors='coerce')\n",
    "\n",
    "# Convert EPS columns to float\n",
    "df['EPS Estimate'] = pd.to_numeric(df['EPS Estimate'], errors='coerce')\n",
    "df['Reported EPS'] = pd.to_numeric(df['Reported EPS'], errors='coerce')\n",
    "\n",
    "# Filter for positive surprises\n",
    "positive_surprises = df[\n",
    "    (df['Reported EPS'] > df['EPS Estimate']) &\n",
    "    df['Earnings Date'].notna()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd68f37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid returns: 36\n",
      "Returns (%): [-2.62, 1.58, 0.79, 4.03, -1.55, 4.19, 0.27, -1.73, -1.24, -0.59, 2.39, 1.16, -4.49, 1.2, 0.4, -0.82, 2.04, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26, 0.26]\n",
      "Median return:  0.26 %\n"
     ]
    }
   ],
   "source": [
    "returns = []\n",
    "\n",
    "for date in positive_surprises['Earnings Date']:\n",
    "    future_dates = amzn[amzn['Date'] > date].reset_index(drop=True)\n",
    "    if len(future_dates) >= 3:\n",
    "        try:\n",
    "            p1 = float(future_dates.loc[0, 'Close_AMZN'])\n",
    "            p3 = float(future_dates.loc[2, 'Close_AMZN'])\n",
    "            if p1 > 0 and p3 > 0:\n",
    "                r = (p3 / p1 - 1) * 100\n",
    "                returns.append(round(r, 2))\n",
    "        except Exception as e:\n",
    "            print(f\"Skipping {date}: {e}\")\n",
    "\n",
    "print(f\"Valid returns: {len(returns)}\")\n",
    "print(\"Returns (%):\", returns)\n",
    "print(\"Median return: \", round(pd.Series(returns).median(), 2), \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
